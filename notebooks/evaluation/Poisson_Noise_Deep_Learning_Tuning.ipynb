{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzBu6o35D8Fv"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/Poisson_Study/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG7prEOihy2y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pandas as pd\n",
        "from skimage.transform import radon, iradon, rescale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from os.path import basename"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.io import imread\n",
        "from skimage.transform import radon, iradon\n",
        "from skimage.filters import gaussian\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Poisson_Study/labels.csv\")\n",
        "\n",
        "def prepare_images(file_paths, df, image_size=(256, 256), theta=np.linspace(0., 180., 180, endpoint=False), apply_sinogram=True):\n",
        "    img_array = []\n",
        "    labels = []\n",
        "    for file_path in file_paths:\n",
        "        img = imread(file_path, as_gray=True)\n",
        "        if img is None:\n",
        "            print(\"Failed to load image:\", file_path)\n",
        "            continue\n",
        "\n",
        "        img = cv2.resize(img, image_size)\n",
        "\n",
        "        if apply_sinogram:\n",
        "            sinogram = radon(img, theta=theta)\n",
        "            scale_factor = 0.5\n",
        "            noisy_sinogram = np.random.poisson(sinogram * scale_factor)\n",
        "            reconstructed_image = iradon(noisy_sinogram, theta=theta, filter_name='hamming')\n",
        "            img = reconstructed_image\n",
        "\n",
        "        img = np.expand_dims(img, axis=-1) / 255.0\n",
        "\n",
        "        img_array.append(img)\n",
        "\n",
        "        base_name = os.path.basename(file_path)\n",
        "        label = df[df['filename'] == base_name]['hemorrhage'].values\n",
        "        if len(label) > 0:\n",
        "            labels.append(label[0])\n",
        "        else:\n",
        "            print(f\"No label found for {base_name}; skipping image.\")\n",
        "\n",
        "    return np.array(img_array), np.array(labels)\n",
        "\n",
        "file_paths = glob.glob(\"/content/drive/My Drive/Poisson_Study/data/*.png\")\n",
        "\n",
        "X_train, X_test = train_test_split(file_paths, test_size=0.2, random_state=8)\n",
        "X_test, X_validation = train_test_split(X_test, test_size=0.5, random_state=8)\n",
        "X_train_processed, y_train = prepare_images(X_train, df, apply_sinogram=True)\n",
        "X_test_processed, y_test = prepare_images(X_test, df, apply_sinogram=False)\n",
        "X_validation_processed, y_validation = prepare_images(X_validation, df, apply_sinogram=False)\n",
        "X_train_unprocessed, y_train_unprocessed = prepare_images(X_train, df, apply_sinogram=False)\n",
        "X_train_combined = np.concatenate([X_train_processed, X_train_unprocessed])\n",
        "y_train_combined = np.concatenate([y_train, y_train_unprocessed])\n"
      ],
      "metadata": {
        "id": "J5ItdLaVzbF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "from skimage.io import imread\n",
        "from skimage.transform import radon, iradon\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class Block(tf.keras.Model):\n",
        "    def __init__(self, filters, kernel_size, repetitions, pool_size=2, strides=2):\n",
        "        super(Block, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.repetitions = repetitions\n",
        "        self.conv_layers = [\n",
        "            tf.keras.layers.Conv2D(filters, kernel_size, activation='relu', padding=\"same\")\n",
        "            for _ in range(repetitions)\n",
        "        ]\n",
        "        self.max_pool = tf.keras.layers.MaxPooling2D(pool_size, strides=strides)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for conv in self.conv_layers:\n",
        "            x = conv(x)\n",
        "        return self.max_pool(x)\n",
        "\n",
        "class MyVGG(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyVGG, self).__init__()\n",
        "        self.block_a = Block(filters=64, kernel_size=3, repetitions=2)\n",
        "        self.block_b = Block(filters=128, kernel_size=3, repetitions=2)\n",
        "        self.block_c = Block(filters=256, kernel_size=3, repetitions=3)\n",
        "        self.block_d = Block(filters=512, kernel_size=3, repetitions=3)\n",
        "        self.block_e = Block(filters=512, kernel_size=3, repetitions=3)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, return_last_conv=False):\n",
        "        x = self.block_a(inputs)\n",
        "        x = self.block_b(x)\n",
        "        x = self.block_c(x)\n",
        "        x = self.block_d(x)\n",
        "        x = self.block_e(x)\n",
        "        if return_last_conv:\n",
        "            return x\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def prepare_train_images(file_paths, image_size=(256, 256), theta=np.linspace(0., 180., 180, endpoint=False)):\n",
        "    img_array = []\n",
        "    labels = []\n",
        "    for file_path in file_paths:\n",
        "        base_name = os.path.basename(file_path)\n",
        "\n",
        "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, image_size)\n",
        "            sinogram = radon(img, theta=theta)\n",
        "\n",
        "            scale_factor = 2\n",
        "            noisy_sinogram = np.random.poisson(sinogram * scale_factor)\n",
        "\n",
        "            reconstructed_image = iradon(noisy_sinogram, theta=theta, filter_name='hamming')\n",
        "\n",
        "            reconstructed_image = np.expand_dims(reconstructed_image, axis=-1) / 255.0\n",
        "            img_array.append(reconstructed_image)\n",
        "\n",
        "            label_series = df[df['filename'] == base_name]['hemorrhage']\n",
        "            if not label_series.empty:\n",
        "                labels.append(label_series.iloc[0])\n",
        "            else:\n",
        "                print(f\"No label found for {base_name}; skipping image.\")\n",
        "                continue\n",
        "        else:\n",
        "            print(\"Failed to load image:\", file_path)\n",
        "\n",
        "    return np.array(img_array), np.array(labels)\n",
        "\n",
        "def prepare_test_images(file_paths, image_size=(256, 256)):\n",
        "    img_array = []\n",
        "    labels = []\n",
        "    for file_path in file_paths:\n",
        "        base_name = os.path.basename(file_path)\n",
        "\n",
        "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, image_size)\n",
        "            img = np.expand_dims(img, axis=-1)\n",
        "            img_array.append(img)\n",
        "\n",
        "            label_series = df[df['filename'] == base_name]['hemorrhage']\n",
        "            if not label_series.empty:\n",
        "                labels.append(label_series.iloc[0])\n",
        "            else:\n",
        "                print(f\"No label found for {base_name}; skipping image.\")\n",
        "                continue\n",
        "        else:\n",
        "            print(\"Failed to load image:\", file_path)\n",
        "\n",
        "    return np.array(img_array) / 255.0, np.array(labels)\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Poisson_Study/labels.csv\")\n",
        "file_paths = glob.glob(\"/content/drive/My Drive/Poisson_Study/data/*.png\")\n",
        "X_train_files, X_temp = train_test_split(file_paths, test_size=0.2, random_state=8)\n",
        "X_validation_files, X_test_files = train_test_split(X_temp, test_size=0.5, random_state=8)\n",
        "X_train_processed, y_train = prepare_train_images(X_train_files)\n",
        "\n",
        "X_test, y_test = prepare_test_images(X_test_files)\n",
        "X_validation, y_validation = prepare_test_images(X_validation_files)\n",
        "\n",
        "X_train_combined, y_train_combined = prepare_train_images(X_train_files)\n",
        "learning_rate = 0.001\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 1e-07\n",
        "\n",
        "optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n",
        "model = MyVGG(num_classes=2)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(X_train_combined, y_train_combined, epochs=20, validation_data=(X_validation, y_validation))\n"
      ],
      "metadata": {
        "id": "zL7CLEjjzgXi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzBu6o35D8Fv",
        "outputId": "16fa2d53-2ad2-4662-a13a-b43860e79994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "\n",
        "# # # Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# # Change the directory to the location of the data\n",
        "os.chdir(\"/content/drive/My Drive/Poisson_Study/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wG7prEOihy2y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import glob\n",
        "import pandas as pd\n",
        "from skimage.transform import radon, iradon, rescale\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from os.path import basename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yOhXXPrjiyw4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Poisson_Study/labels.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P6he2EMwh6G-"
      },
      "outputs": [],
      "source": [
        "def prepare_images(file_paths, image_size=(256, 256), theta=np.linspace(0., 180., 180, endpoint=False)):\n",
        "    img_array = []\n",
        "    labels = []\n",
        "    for file_path in file_paths:\n",
        "        img = imread(file_path, as_gray=True)\n",
        "        if img is None:\n",
        "            print(\"Failed to load image:\", file_path)\n",
        "            continue\n",
        "\n",
        "        img = rescale(img, scale=0.4, mode='reflect', multichannel=False, anti_aliasing=False)\n",
        "        sinogram = radon(img, theta=theta)\n",
        "        ## completed -1,1,5,2,2.5 3,4,10,5\n",
        "\n",
        "        scale_factor = 1.5\n",
        "        noisy_sinogram = np.random.poisson(sinogram * scale_factor)\n",
        "\n",
        "        reconstructed_image = iradon(noisy_sinogram, theta=theta, filter_name='hamming')\n",
        "        reconstructed_image = cv2.resize(reconstructed_image, image_size)\n",
        "\n",
        "        # Normalize the reconstructed image\n",
        "        reconstructed_image = np.expand_dims(reconstructed_image, axis=-1) / 255.0\n",
        "        img_array.append(reconstructed_image)\n",
        "\n",
        "        # Extract label using filename\n",
        "        base_name = basename(file_path)\n",
        "        label_series = df[df['filename'] == base_name]['hemorrhage']\n",
        "        if not label_series.empty:\n",
        "            labels.append(label_series.iloc[0])\n",
        "        else:\n",
        "            print(f\"No label found for {base_name}; skipping image.\")\n",
        "\n",
        "    return np.array(img_array), np.array(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HuhJfwYiG-Q",
        "outputId": "2708a89b-1309-4510-d689-c0c8935cc66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-84757088f9d3>:11: FutureWarning: `multichannel` is a deprecated argument name for `rescale`. It will be removed in version 1.0. Please use `channel_axis` instead.\n",
            "  img = rescale(img, scale=0.4, mode='reflect', multichannel=False, anti_aliasing=False)\n",
            "/usr/local/lib/python3.10/dist-packages/skimage/transform/radon_transform.py:75: UserWarning: Radon transform: image must be zero outside the reconstruction circle\n",
            "  warn('Radon transform: image must be zero outside the '\n"
          ]
        }
      ],
      "source": [
        "file_paths = glob.glob(\"/content/drive/My Drive/Poisson_Study/data_large/*.png\")\n",
        "X, y = prepare_images(file_paths)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-TnX1o7CpOL",
        "outputId": "fa7fca41-360f-4661-9bce-d0881a69903a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 11/100 [==>...........................] - ETA: 2:13:40 - loss: 0.9129 - accuracy: 0.5057"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "class Block(tf.keras.Model):\n",
        "    def __init__(self, filters, kernel_size, repetitions, pool_size=2, strides=2):\n",
        "        super(Block, self).__init__()\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.repetitions = repetitions\n",
        "        self.conv_layers = [\n",
        "            tf.keras.layers.Conv2D(filters, kernel_size, activation='relu', padding=\"same\")\n",
        "            for _ in range(repetitions)\n",
        "        ]\n",
        "        self.max_pool = tf.keras.layers.MaxPooling2D(pool_size, strides=strides)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for conv in self.conv_layers:\n",
        "            x = conv(x)\n",
        "        return self.max_pool(x)\n",
        "\n",
        "class MyVGG(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(MyVGG, self).__init__()\n",
        "        self.block_a = Block(filters=64, kernel_size=3, repetitions=2)\n",
        "        self.block_b = Block(filters=128, kernel_size=3, repetitions=2)\n",
        "        self.block_c = Block(filters=256, kernel_size=3, repetitions=3)\n",
        "        self.block_d = Block(filters=512, kernel_size=3, repetitions=3)\n",
        "        self.block_e = Block(filters=512, kernel_size=3, repetitions=3)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(256, activation='relu')\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, return_last_conv=False):\n",
        "        x = self.block_a(inputs)\n",
        "        x = self.block_b(x)\n",
        "        x = self.block_c(x)\n",
        "        x = self.block_d(x)\n",
        "        x = self.block_e(x)  # Last convolutional output\n",
        "        if return_last_conv:\n",
        "            return x\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def call(self, inputs):\n",
        "        x = self.block_a(inputs)\n",
        "        x = self.block_b(x)\n",
        "        x = self.block_c(x)\n",
        "        x = self.block_d(x)\n",
        "        x = self.block_e(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def prepare_images(file_paths, image_size=(256, 256)):\n",
        "    img_array = []\n",
        "    labels = []\n",
        "    for file_path in file_paths:\n",
        "        base_name = os.path.basename(file_path)\n",
        "\n",
        "        # Read and resize the image\n",
        "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, image_size)\n",
        "            img = np.expand_dims(img, axis=-1)  # Add channel dimension for grayscale\n",
        "            img_array.append(img)\n",
        "\n",
        "            # Find the corresponding label using the filename\n",
        "            label_series = df[df['filename'] == base_name]['hemorrhage']\n",
        "            if not label_series.empty:\n",
        "                labels.append(label_series.iloc[0])\n",
        "            else:\n",
        "                print(f\"No label found for {base_name}; skipping image.\")\n",
        "                continue  # Skip this image if no label found\n",
        "        else:\n",
        "            print(\"Failed to load image:\", file_path)\n",
        "\n",
        "    return np.array(img_array) / 255.0, np.array(labels)\n",
        "\n",
        "\n",
        "\n",
        "# # Retrieve image file paths\n",
        "# file_paths = glob.glob(\"/content/drive/My Drive/Poisson_Study/data/*.png\")\n",
        "\n",
        "# # Prepare images and labels\n",
        "# X, y = prepare_images(file_paths)\n",
        "\n",
        "# # Split data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "\n",
        "model = MyVGG(num_classes=2)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=1, validation_data=(X_test, y_test))\n",
        "model = MyVGG(num_classes=2)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvyeKtcCeknH"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/Poisson_Study/vgg.h5\")\n",
        "y_pred = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUl88forgald"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = [np.argmax(a) for a in y_pred]\n",
        "\n",
        "print(\"Accuracy of model = \", accuracy_score(y_test, y_pred_binary))\n",
        "print(classification_report(y_test, y_pred_binary, target_names=['0', '1']))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}